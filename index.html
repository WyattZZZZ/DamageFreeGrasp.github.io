<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Damage-Free Grasping</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Generic Framework for Damage-Free Grasping of Delicate Produce
              Using LLMs and Volume Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="ZIYE_ZHANG_PERSONAL_LINK" target="_blank">Ziye Zhang</a><sup
                    class="equal-contribution">*</sup><sup class="affiliation-a">b</sup>,
                </span>
                <span class="author-block">
                  <a href="XIAOYU_XIA_PERSONAL_LINK" target="_blank">Xiaoyu Xia</a><sup
                    class="equal-contribution">b,*</sup><sup class="affiliation-a">b</sup>,
                </span>
                <span class="author-block">
                  <a href="YUHAO_JIN_PERSONAL_LINK" target="_blank">Yuhao Jin</a><sup class="affiliation-a">a,b</sup>,
                </span>
                <span class="author-block">
                  <a href="QIZHONG_GAO_PERSONAL_LINK" target="_blank">Qizhong Gao</a><sup
                    class="affiliation-a">a,b</sup>,
                </span>
                <span class="author-block">
                  <a href="LIN_QIAO_PERSONAL_LINK" target="_blank">Lin Qiao</a><sup class="affiliation-a">a,b</sup><br>
                </span>
                <span class="author-block">
                  <a href="JINGLEI_CHEN_PERSONAL_LINK" target="_blank">Jinglei Chen</a><sup
                    class="affiliation-b">b</sup>,
                </span>
                <span class="author-block">
                  <a href="YONG_YUE_PERSONAL_LINK" target="_blank">Yong Yue</a><sup class="affiliation-b">b</sup>,
                </span>
                <span class="author-block">
                  <a href="SHANLIANG_YAO_PERSONAL_LINK" target="_blank">ShanLiang Yao</a><sup
                    class="affiliation-c">c</sup>,
                </span>
                <span class="author-block">
                  <a href="XIAOHUI_ZHU_PERSONAL_LINK" target="_blank">Xiaohui Zhu</a><sup
                    class="affiliation-a">a</sup><sup class="corresponding-author">†</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Department of Computer Science, University of Liverpool, Liverpool, UK<sup
                    class="affiliation-a">a</sup><br>
                  School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China<sup
                    class="affiliation-b">b</sup><br>
                  School of Information Engineering, YanCheng Institute of Technology, Yancheng, China<sup
                    class="affiliation-c">c</sup>
                </span>
                <span class="eql-cntrb" style="font-size: 0.8em;">
                  <small><br><sup class="equal-contribution">*</sup> Indicates Equal Contribution</small>
                </span>
                <span class="corr-auth" style="font-size: 0.8em;">
                  <small><br><sup class="corresponding-author">†</sup> Corresponding Author</small>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>GitHub</span>
                    </a>
                  </span>
                  <p style="font-size: 0.8em; margin-top: 5px;">* Code repository will be released after paper acceptance.</p>
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Supplementary PDF link -->
                  <span class="link-block">
                    <a href="static/pdfs/Data Record.xlsx" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/7.png" />
        <h2 class="subtitle has-text-centered">
          Demonstration of front and side views for non-destructive grasping of several representative crops.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite notable progress in agricultural robotics, achieving stable, adaptive, and damage-free grasping of
              irregular, fragile, and diverse produce remains a key unsolved challenge. Traditional approaches rely on
              geometric heuristics or fixed-force strategies, which cannot accurately model or adapt to the complex
              physical properties of various produce, often causing instability or damage. To the best of our knowledge,
              this paper proposes the first adaptive damage-free grasping framework that integrates large language
              models (LLMs) with multimodal perception to produce. We develop an intelligent framework combining
              semantic understanding from visual images, geometric awareness from 3D point clouds, and LLMs commonsense
              knowledge to infer the minimal stable grasping force, enhancing safety and adaptability. To enhance
              accuracy and robustness, we introduce a point-cloud-based volume estimation module that directly leverages
              spatial geometry, thereby minimizing reliance on LLMs reasoning and significantly improving perception
              quality and estimation precision. Moreover, we design a hybrid multi-agent collaboration mechanism that
              efficiently coordinates perception, reasoning, and control agents, improving reasoning and execution in
              complex environments. Experiments on a custom dataset of 25 typical produce categories show our framework
              significantly outperforms state-of-the-art baselines in both volume estimation accuracy and damage-free
              grasp success rate, demonstrating excellent stability, robustness, and truly damage-free capability. This
              work validates the effectiveness of combining LLMs and multimodal perception in agricultural robotics and
              suggests a promising direction for future research toward intelligent and reliable grasping.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/a_piece_of_lettuce.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              A piece of lettuce
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/banana.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Banana
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/grape.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Grape
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/lettuce.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Lettuce
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/mushroom.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Mushroom
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/nec.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Nectarine
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/orange.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Orange
            </h2>
          </div>
          <div class="item">
            <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT"
              style="width: 50%; display: block; margin: 0 auto;">
              <source src="static/videos/pepper.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Pepper
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->




  <!-- Paper Model -->
  <section class="section hero is-small is-light">

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Structure</h2>
          <div class="content has-text-justified">
            <div class="content has-text-justified has-text-centered">
              <img src="static/images/WechatIMG96.jpg" />
              <p>
                This study presents a new framework for damage-free grasping of delicate produce, merging 3D point cloud
                geometric perception with large language models (LLMs). This innovative approach, the first of its kind
                in agricultural robotics, improves grasping adaptability and accuracy by combining visual perception
                with language-based reasoning. The system has three main modules. First, a 3D perception module uses
                advanced visual models to process RGB images and depth maps, extracting 3D geometric information. Next,
                a physical property reasoning module leverages LLMs to infer crucial physical properties like density
                and friction coefficients from semantic labels and geometric data. Finally, a grasp strategy generation
                and execution module uses the reasoned properties to calculate the minimum required force and generate
                optimal grasp poses, which are then executed by a force-controlled gripper. This integrated system
                ensures stable, damage-free grasping across various types of produce.
              </p>
            </div>
          </div>
        </div>
      </div>
  </section>
  <!-- End paper model -->


  <!-- Paper results -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
          <p>
            The "Experiments" section aims to evaluate the proposed multimodal damage-free grasping framework, focusing
            on LLMs reasoning-based performance, grasp force control, and comparing the effectiveness of different
            estimation pathways. The experiments were conducted on a custom dataset comprising 25 typical produce
            categories. Overall, the framework significantly outperforms state-of-the-art baselines in both volume
            estimation accuracy and damage-free grasp success rate.
          </p>
          <br>
          <h3 class="title is-4">Verification of framework validity</h3>
          <img src="static/images/WechatIMG94.jpg" />
          <div class="content has-text-justified">
            <br>
            <table style="width: 100%; border-collapse: collapse;">
              <tr>
                <td style="width: 60%; vertical-align: top; padding-right: 20px;">
                  <p>The framework's validity is demonstrated by its success in achieving truly damage-free and stable
                    grasping across diverse, fragile produce. The key findings include:</p>
                  <ul>
                    <li>The system's core component, the <strong>point-cloud-based volume estimation module</strong>,
                      successfully reduces reliance on LLMs' general commonsense reasoning by directly leveraging
                      spatial geometry, leading to significantly improved perception quality and estimation precision.
                    </li>
                    <li>A <strong>hybrid multi-agent collaboration mechanism</strong> efficiently coordinates the
                      perception, reasoning, and control agents. This collaboration improves the framework's overall
                      efficiency and general applicability in complex perception and parameter reasoning tasks.</li>
                    <li>The system demonstrated <strong>excellent stability and robustness</strong> across a custom
                      dataset of 25 typical produce categories, validating its damage-free capability.</li>
                  </ul>
                </td>

                <td style="width: 40%; vertical-align: top; text-align: center;">
                  <img src="static/images/5.png" alt="Framework Diagram" style="max-width: 100%; height: auto;">
                </td>
              </tr>
            </table>
          </div>

          <br>
          <h3 class="title is-4">Our model VS Others</h3>

          <div class="content has-text-justified">
            <img src="static/images/WechatIMG95.jpg" />
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                <p>Our framework is positioned as the first adaptive damage-free grasping solution that integrates Large
                  Language Models (LLMs) with multimodal perception for produce.</p>
                <ul>
                  <li><strong>Key Advantage:</strong> Unlike traditional approaches that rely on geometric heuristics or
                    fixed-force strategies, which often cause instability or damage, your model intelligently infers the
                    minimal stable grasping force.</li>
                  <li><strong>Quantitative Outperformance:</strong> The framework outperform in two critical metrics:
                    volume estimation accuracy and damage-free grasp success rate.</li>
                  <li><strong>Validation:</strong> This work validates the effectiveness of combining LLMs and
                    multimodal perception, offering an adaptable and generalizable solution that ensures stable grasping
                    while actively preventing damage.</li>
                </ul>
                </p>
              </tr>
            </table>

          </div>

          <br>
  </section>
  <!-- End paper experiment -->


  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/A_Generic_Workflow_for_Zero_Damage_Grasping_of_Delicate_Produce_Using_LLMs_and_Volume_Estimation.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{DaFrGrasp,
  title={A Generic Framework for Damage-Free Grasping of Delicate Produce Using LLMs and Volume Estimation},
  author={Zhang, Ziye and Xia, Xiaoyu and Jin, Yuhao and Gao, Qizhong and Qiao, Lin and Chen, Jinglei and Yue, Yong and Yao, ShanLiang and Zhu, Xiaohui},
  year={2025},
  doi={}
}
</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

</body>

</html>