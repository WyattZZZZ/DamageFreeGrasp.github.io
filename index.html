<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Damage-Free Grasping</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Generic Framework for Damage-Free Grasping of Delicate Produce Using LLMs and Volume Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <div class="is-size-5 publication-authors">
  <span class="author-block">
    <a href="ZIYE_ZHANG_PERSONAL_LINK" target="_blank">Ziye Zhang</a><sup class="equal-contribution">*</sup><sup class="affiliation-a">b</sup>,
  </span>
  <span class="author-block">
    <a href="XIAOYU_XIA_PERSONAL_LINK" target="_blank">Xiaoyu Xia</a><sup class="equal-contribution">b,*</sup><sup class="affiliation-a">b</sup>,
  </span>
  <span class="author-block">
    <a href="YUHAO_JIN_PERSONAL_LINK" target="_blank">Yuhao Jin</a><sup class="affiliation-a">a,b</sup>,
  </span>
  <span class="author-block">
    <a href="QIZHONG_GAO_PERSONAL_LINK" target="_blank">Qizhong Gao</a><sup class="affiliation-a">a,b</sup>,
  </span>
  <span class="author-block">
    <a href="LIN_QIAO_PERSONAL_LINK" target="_blank">Lin Qiao</a><sup class="affiliation-a">a,b</sup><br>
  </span>
  <span class="author-block">
    <a href="JINGLEI_CHEN_PERSONAL_LINK" target="_blank">Jinglei Chen</a><sup class="affiliation-b">b</sup>,
  </span>
  <span class="author-block">
    <a href="YONG_YUE_PERSONAL_LINK" target="_blank">Yong Yue</a><sup class="affiliation-b">b</sup>,
  </span>
  <span class="author-block">
    <a href="SHANLIANG_YAO_PERSONAL_LINK" target="_blank">ShanLiang Yao</a><sup class="affiliation-c">c</sup>,
  </span>
  <span class="author-block">
    <a href="XIAOHUI_ZHU_PERSONAL_LINK" target="_blank">Xiaohui Zhu</a><sup class="affiliation-a">a</sup><sup class="corresponding-author">†</sup>
  </span>
</div>

<div class="is-size-5 publication-authors">
  <span class="author-block">
    Department of Computer Science, University of Liverpool, Liverpool, UK<sup class="affiliation-a">a</sup><br>
    School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China<sup class="affiliation-b">b</sup><br>
    School of Information Engineering, YanCheng Institute of Technology, Yancheng, China<sup class="affiliation-c">c</sup>
  </span>
<span class="eql-cntrb" style="font-size: 0.8em;">
  <small><br><sup class="equal-contribution">*</sup> Indicates Equal Contribution</small>
</span>
<span class="corr-auth" style="font-size: 0.8em;">
  <small><br><sup class="corresponding-author">†</sup> Corresponding Author</small>
</span>
</div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="pdfs/A_Generic_Workflow_for_Zero_Damage_Grasping_of_Delicate_Produce_Using_LLMs_and_Volume_Estimation.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/Data Record.xlsx" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/7.png" />
      <h2 class="subtitle has-text-centered">
        Demonstration of front and side views for non-destructive grasping of several representative crops.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite notable progress in agricultural robotics, achieving stable, adaptive, and damage-free grasping of irregular, fragile, and diverse produce remains a key unsolved challenge. Traditional approaches rely on geometric heuristics or fixed-force strategies, which cannot accurately model or adapt to the complex physical properties of various produce, often causing instability or damage. To the best of our knowledge, this paper proposes the first adaptive damage-free grasping framework that integrates large language models (LLMs) with multimodal perception to produce. We develop an intelligent framework combining semantic understanding from visual images, geometric awareness from 3D point clouds, and LLMs commonsense knowledge to infer the minimal stable grasping force, enhancing safety and adaptability. To enhance accuracy and robustness, we introduce a point-cloud-based volume estimation module that directly leverages spatial geometry, thereby minimizing reliance on LLMs reasoning and significantly improving perception quality and estimation precision. Moreover, we design a hybrid multi-agent collaboration mechanism that efficiently coordinates perception, reasoning, and control agents, improving reasoning and execution in complex environments. Experiments on a custom dataset of 25 typical produce categories show our framework significantly outperforms state-of-the-art baselines in both volume estimation accuracy and damage-free grasp success rate, demonstrating excellent stability, robustness, and truly damage-free capability. This work validates the effectiveness of combining LLMs and multimodal perception in agricultural robotics and suggests a promising direction for future research toward intelligent and reliable grasping.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
          <source src="static/videos/a_piece_of_lettuce.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          A piece of lettuce
        </h2>
      </div>
      <div class="item">
        <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
          <source src="static/videos/banana.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Banana
        </h2>
      </div>
      <div class="item">
        <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
          <source src="static/videos/grape.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
         Grape
       </h2>
     </div>
     <div class="item">
      <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
        <source src="static/videos/lettuce.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       Lettuce
     </h2>
   </div>
   <div class="item">
    <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
      <source src="static/videos/mushroom.mp4"
      type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
     Mushroom
   </h2>
 </div>
  <div class="item">
    <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
      <source src="static/videos/nec.mp4"
      type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
     Nectarine
   </h2>
 </div>
   <div class="item">
    <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
      <source src="static/videos/orange.mp4"
      type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
     Orange
   </h2>
 </div>
   <div class="item">
    <video poster="" id="tree" autoplay controls muted loop height="100%" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto;">
      <source src="static/videos/pepper.mp4"
      type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
     Pepper
   </h2>
 </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Paper Model -->
<section class="section hero is-small is-light">
  
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Structure</h2>
        <div class="content has-text-justified">
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/WechatIMG96.jpg" />
          <p>
            This study presents a new framework for damage-free grasping of delicate produce, merging 3D point cloud geometric perception with large language models (LLMs). This innovative approach, the first of its kind in agricultural robotics, improves grasping adaptability and accuracy by combining visual perception with language-based reasoning. The system has three main modules. First, a 3D perception module uses advanced visual models to process RGB images and depth maps, extracting 3D geometric information. Next, a physical property reasoning module leverages LLMs to infer crucial physical properties like density and friction coefficients from semantic labels and geometric data. Finally, a grasp strategy generation and execution module uses the reasoned properties to calculate the minimum required force and generate optimal grasp poses, which are then executed by a force-controlled gripper. This integrated system ensures stable, damage-free grasping across various types of produce.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper model -->


<!-- Paper results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>

          <h3 class="title is-4">Verification of framework validity</h3>
          <img src="static/images/WechatIMG94.jpg" />
          <div class="content has-text-justified">

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                  We evaluate OpenVLA's ability to control multiple robot platforms ``out-of-the-box'' across two setups: the WidowX setup 
                  from Bridge V2 and the Google Robot from the RT-series of papers. Our results show that OpenVLA sets a new state of the art, 
                  outperforming prior generalist policies RT-1-X and Octo. Notably, as a product of the added data diversity and new model
                  components, it also outperforms RT-2-X, a 55B parameter closed VLA.
                </p>
              </tr>
              <tr>
                <td width="50%">
                  <p>
                    We test OpenVLA across a wide range of generalization tasks, such as 
                    <strong>visual</strong> (unseen backgrounds, distractor objects, colors/appearances of objects); 
                    <strong>motion</strong> (unseen object positions/orientations); 
                    <strong>physical</strong> (unseen object sizes/shapes); 
                    and <strong>semantic</strong> (unseen target objects, instructions, and concepts from the Internet) generalization. 
                    Qualitatively, we find that both RT-2-X and OpenVLA exhibit markedly more robust behaviors than the other tested model, 
                    such as approaching the correct object when distractor objects are present, properly orienting the robot's end-effector
                    to align with the orientation of the target object, and even recovering from mistakes such as insecurely grasping objects
                  </p>
                </td>
                <td width="50%">
                  <img src="static/images/5.png">
                </td>
              </tr>
            </table>
          </div>

          <br>
          <h3 class="title is-4">Our model VS Others</h3>
          
          <div class="content has-text-justified">
            <img src="static/images/WechatIMG95.jpg" />
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                  Effective fine-tuning of VLA models to new tasks and robot setups is largely unexplored, 
                  yet is key for their widespread adoption. We investigate OpenVLA’s ability to be quickly adapted to a new robot setup
                  in two domains: Franka-Tabletop, a stationary, table-mounted Franka Emika Panda 7-DoF robot arm, controlled 
                  at a frequency of 5 Hz; and Franka-DROID, the Franka robot arm setup from the recently released <a href="https://droid-dataset.github.io/">DROID dataset</a>, controlled at 15 Hz. 

                </p>
              </tr>
              <tr>
                <p>
                  We compare to Diffusion Policy, a state of the art data-efficient imitation learning approach, trained from scratch.
                  Additionally, we evaluate Octo fine-tuned on the target dataset. OpenVLA clearly outperforms Octo across most tasks.
                  Diffusion policy is strongest on narrower, more precise tasks, while OpenVLA shows better performance on tasks that 
                  require grounding language to behavior in multi-task, multi-object settings.
                  OpenVLA is the only approach that achieves at least 50% success rate across all tested
                  tasks, suggesting that it can be a strong default option for imitation learning tasks, particularly if they
                  involve a diverse set of language instructions.
                </p>
              </tr>
            </table>
            
          </div>

        <br>
        </section>
<!-- End paper experiment -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/A_Generic_Workflow_for_Zero_Damage_Grasping_of_Delicate_Produce_Using_LLMs_and_Volume_Estimation.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{DaFrGrasp,
  title={A Generic Framework for Damage-Free Grasping of Delicate Produce Using LLMs and Volume Estimation},
  author={Zhang, Ziye and Xia, Xiaoyu and Jin, Yuhao and Gao, Qizhong and Qiao, Lin and Chen, Jinglei and Yue, Yong and Yao, ShanLiang and Zhu, Xiaohui},
  year={2025},
  doi={}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
